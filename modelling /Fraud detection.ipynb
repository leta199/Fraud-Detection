{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595d0f8e",
   "metadata": {},
   "source": [
    "# FRAUD DETECTION \n",
    "\n",
    "A classic problem in financial environments. We will take a look at classifying transitions into fraud and not fraud using scikit-learn package.\n",
    "\n",
    "This notebook will be using a synthetic database found on kaggle for educational purposes beacuse we all need practice :-).\n",
    "I will break this proces down into the follwoing steps:\n",
    "\n",
    "1. Data Exploration \n",
    "2. Data prepartion and pre-processing \n",
    "3. Modelling \n",
    "4. Evaluation and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09546d1",
   "metadata": {},
   "source": [
    "We have the following fields within our dataset: \n",
    "\n",
    "transaction_id - identifier to each transaction   \n",
    "user_id - identifier for each user.   \n",
    "transaction_amount - amount for each transaction.   \n",
    "transaction_type - how funds were exchanged e.g \"payment\" or \"bank transfer\".   \n",
    "payment_mode - wallet, card, UPI etc.   \n",
    "device_type - device transaction was made from e.g iOS.   \n",
    "device_location - location of the device used to make transaction.   \n",
    "account_age_days - age of the account     \n",
    "transaction_hour - time of transaction in 24 hour notation.    \n",
    "previous_failed_attempts - if there were previous attempts to make fraudulent transactions   \n",
    "avg_transaction_amount - avg amount each account usually makes   \n",
    "is_international - is the trasnaction international    \n",
    "ip_risk_score - a numerical value  that quantifies the likelihood an IP address is involved in malicious activity, such as fraud, spam, or cyberattacks. \n",
    "login_attempts_last_24h - number of login attempts to the account in the last 24 hours   \n",
    "fraud_label- is the transaction fraud or not \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0223c",
   "metadata": {},
   "source": [
    "## Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68949ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to start by acquiring all of our dependecies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b578585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will load our data in \n",
    "fraud = pd.read_csv('/Users/leta/Desktop/Data Science Career /Python/Python Projects/Fraud Detection /dataset/Digital_Payment_Fraud_Detection_Dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us take a look into the data we have \n",
    "fraud.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this dataset we have mostly numerical variables with a few catgeroical variables.\n",
    "#Let us look at what fields we have\n",
    "fraud.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What unique labels do we have in the dataset \n",
    "fraud.nunique()\n",
    "# The fields with the highest unique values are the ID fields, transaction amounts, account ages, avgerage transaction amounts, ip risk scores \n",
    "# ID fields are just unqiue ientifiers but have no bearing on prediction therefore we will drop these fields especially since they have many unique values.  \n",
    "# We will plore some more of these fields to see there bearing on the predcitve power of our models using exploratory data analysis and data mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30457981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let us see of the data is balanced or not \n",
    "fraud[\"fraud_label\"].value_counts(normalize=True). \n",
    "# Fraud - 6.52% of observations    \n",
    "#Non fraud - 93.28% of observations \n",
    "# Our data is very skewed towards non fraud transactions.\n",
    "# This will infrom our metric to measure perfomrance (probably balanced accuarcy, recall and precision) \n",
    "# as well as how we stratify the data when partitioning into test and train sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54c85f",
   "metadata": {},
   "source": [
    "Our variables with categorical information being: transaction_typ, payment_mode\tand device_type\tdevice_location do not have too many unqiue values.  \n",
    "This infroms what methods we can use to deal with these values for classification models that only use numerical data.  \n",
    "- A possible solution ould be to use dummy variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24b084",
   "metadata": {},
   "source": [
    "##  DATA PREPARATION AND PRE-PROCESSING \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have seen above the ID variables have no bearin on the prediction so we will remove them.\n",
    "fraud = fraud.drop(columns= [\"transaction_id\", \"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING \n",
    "# Since we want to use scikit-learn binary classification methods, the simplest way is to use dummy varibles for non-numeric variables \n",
    "df1 = pd.get_dummies( data = fraud, #what data we want ot get dummies variables of \n",
    "                     columns = [\"transaction_type\", \"payment_mode\", \"device_type\", \"device_location\"],  #the non-numeri columns we will convert into dummies\n",
    "                     dtype=int) #turning dummies from True/ False into binary 1/0. \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd962617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will look at the dimensions of the dataframe to see how many more predictors we have added\n",
    "df1.shape # we have added 11 new columns, not too many "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275af0b",
   "metadata": {},
   "source": [
    "## MODELLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc49347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start modelling our data usinf sklearn.\n",
    "#First we download dependencies\n",
    "from sklearn.linear_model import LogisticRegression #this model acts as our basline model for binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will take partition the data into dependent and indepenedt variables. \n",
    "x =df1[[ 'transaction_amount', 'account_age_days',\n",
    "       'transaction_hour', 'previous_failed_attempts',\n",
    "       'avg_transaction_amount', 'is_international', 'ip_risk_score',\n",
    "       'login_attempts_last_24h', 'transaction_type_Payment',\n",
    "       'transaction_type_Transfer', 'transaction_type_Withdrawal',\n",
    "       'payment_mode_Card', 'payment_mode_NetBanking', 'payment_mode_UPI',\n",
    "       'payment_mode_Wallet', 'device_type_Android', 'device_type_Web',\n",
    "       'device_type_iOS', 'device_location_Bangalore',\n",
    "       'device_location_Chennai', 'device_location_Delhi',\n",
    "       'device_location_Hyderabad', 'device_location_Mumbai']]\n",
    "y = df1[['fraud_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3eea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning our data \n",
    "# The data will be partitioned into tran and test splits at a 70/30% proportion \n",
    "\n",
    "from sklearn.model_selection import train_test_split #package to spilt our data \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,  #our dependent and independent variables to be used\n",
    "                                                    stratify = y, #keeping proportion of fraud and not fraud equal in the the train and test sets\n",
    "                                                    random_state=123, #setting a seed for reproducibility \n",
    "                                                    test_size=0.3) #30% test size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we ca n fit the data \n",
    "logreg = LogisticRegression() #assining our model to a variables we will call later \n",
    "logreg.fit(x_train, y_train) #fit our training data to the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec19a0a",
   "metadata": {},
   "source": [
    "## EVALUATION AND TESTING \n",
    "We have trained our model on the train set.   \n",
    "We can now use the metrics mentined earlier like balanced accuracy, precision and recall to judge our models\n",
    "\n",
    "**Presicion** - of all the positievs identified how many were truly positive?  \n",
    "**Recall**- of all possible positive instances, how many did the model catch? \n",
    "\n",
    "For fraud detection, we value recall as our main metric.   \n",
    "This is beacuse catching every fraudlent trasaction allows for the least damage to our client base as opposed to flagging a genuine transaction as fraud that can be reversed wuth no harm to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34955e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(x_test) #making predictios on the train data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing balanced accuracy\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score, precision_recall_curve, confusion_matrix,precision_score, recall_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583af2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred) #We have a very high accuracy score of 0.935\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654638dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,y_pred) #our balaned accuracy score is very lowe at 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,y_pred) # precision is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,y_pred) #recall is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db447b6",
   "metadata": {},
   "source": [
    "This indicates that the imbalance in the dataset even after stratifying the data.     \n",
    "This indicates that our model learned to predict non fraud every time (class 0) to get a high accuracy bot lowe balanced accuracy, precision and recall \n",
    "Therefore we need to find another way to ensure we can identify all the positive cases. \n",
    "# We can award a greater penalty to predicting a very imbalanced outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogisticRegression(class_weight= \"balanced\") #adding penatly to predciting only one class accuractely \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd09092",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = logreg2.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_pred2) #Our model still does not have the best balanced accuracy at 0.525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90db421",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred2) # precision is alos very low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred2) #however, our recall is much higher therefore, we have caught more our our fruad cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_log = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred2\n",
    ")\n",
    "confusion_matrix.ax_.set_title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7568455",
   "metadata": {},
   "source": [
    "In conculsion - even after we add extra penalty to predict each class as good as possible. \n",
    "We will now try to use another classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will try to use a Random Forest Model \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_for = RandomForestClassifier(class_weight= \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e432edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_for.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603129c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rando = random_for.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_pred_rando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d50e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_rando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5742bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_rando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_rando = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_rando\n",
    ")\n",
    "confusion_matrix.ax_.set_title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c884c0",
   "metadata": {},
   "source": [
    "Both our models minimise error by just predicting not fraud for all the transaction since the data is so skewed in not fraud approximatley 94/6 %.\n",
    "Therefore since we have such unbalanced data we have to give a much larger penatly to make our models predict both classes more accuractely.   \n",
    "\n",
    "## MODEL WITH GREATER PENALTY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "57657c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create both models with a greater penatly for predicting wrong in our fraud class as well as lower our acceptance threshold for fraud.\n",
    "Log = LogisticRegression(class_weight= \"balanced\") #a mistake in our fraud class (class 1) is 50 times more detrimental than predicting wrong for class 1\n",
    "Random = RandomForestClassifier(class_weight= \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a676f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leta/Desktop/Data Science Career /Python/Python Projects/Fraud Detection /fraud detect/fraud-detect/lib/python3.14/site-packages/sklearn/utils/validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/leta/Desktop/Data Science Career /Python/Python Projects/Fraud Detection /fraud detect/fraud-detect/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/leta/Desktop/Data Science Career /Python/Python Projects/Fraud Detection /fraud detect/fraud-detect/lib/python3.14/site-packages/sklearn/base.py:1336: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "algos = [Log, Random]\n",
    "b_accuracy =[]\n",
    "recall_metric = []\n",
    "precision_metric = []\n",
    "threshold = 0.1\n",
    "\n",
    "for a in algos: \n",
    "    a.fit(x_train, y_train)\n",
    "    predictions = (a.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "    acc = balanced_accuracy_score(y_test, predictions)\n",
    "    prec = precision_score(y_test, predictions)\n",
    "    rec = recall_score(y_test, predictions)\n",
    "    \n",
    "    b_accuracy.append(acc)\n",
    "    recall_metric.append(rec)\n",
    "    precision_metric.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dc9a5edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced accuracy</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.519520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.065333</td>\n",
       "      <td>0.078014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Description  Logistic Regression  Random Forest\n",
       "0  Balanced accuracy             0.500000       0.519520\n",
       "1             Recall             1.000000       0.224490\n",
       "2          Precision             0.065333       0.078014"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_metrics =pd.DataFrame()\n",
    "main_metrics[\"Description\"] = [ \"Balanced accuracy\", \"Recall\", \"Precision\"]\n",
    "main_metrics[\"Logistic Regression\"] = [b_accuracy[0], recall_metric[0], precision_metric[0]]\n",
    "main_metrics[\"Random Forest\"] = [b_accuracy[1], recall_metric[1], precision_metric[1]]\n",
    "\n",
    "\n",
    "main_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a671b42",
   "metadata": {},
   "source": [
    "Even after using a lower threshold for classification we still have a very low balanced accuracy for oth models at around 50% as well as a low recal score for Random Forest 22.4% and a high recall for Logistic regression 100%. \n",
    "\n",
    "Although the high recall for Logistci Regression may seem good, it does so by over-predicting fraud therefore we trade off our balanced accuracy anf precision. Too many transactions are being flagged as fraud "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detect (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
